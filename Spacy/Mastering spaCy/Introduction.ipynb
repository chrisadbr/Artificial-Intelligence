{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d090bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497b3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bbce804",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I own a ginger cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585ddffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b010a548cc7849c6a6982a771bdb0dce-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">own</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">ginger</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b010a548cc7849c6a6982a771bdb0dce-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b010a548cc7849c6a6982a771bdb0dce-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b010a548cc7849c6a6982a771bdb0dce-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b010a548cc7849c6a6982a771bdb0dce-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b010a548cc7849c6a6982a771bdb0dce-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b010a548cc7849c6a6982a771bdb0dce-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b010a548cc7849c6a6982a771bdb0dce-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b010a548cc7849c6a6982a771bdb0dce-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a25d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill Gates\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is the CEO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc2 = nlp('Bill Gates is the CEO of Microsoft')\n",
    "displacy.render(doc2, style = 'ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc2d32",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa69a9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'went', 'to', 'the', 'market']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3 = nlp(\"I went to the market\")\n",
    "token1 = [token.text for token in doc3]\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e69b0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bill', 'Gates', 'is', 'the', 'CEO', 'of', 'Microsoft']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = [token.text for token in doc2]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a599e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.reuters.com/business/futures-rise-after-biden-xi-call-oil-bounce-2021-09-10/\n",
    "text = '''\n",
    "Sept 10 (Reuters) - Wall Street's main indexes were subdued on Friday as signs of higher inflation and a drop in Apple shares following an unfavorable court ruling offset expectations of an easing in U.S.-China tensions.\n",
    "\n",
    "Data earlier in the day showed U.S. producer prices rose solidly in August, leading to the biggest annual gain in nearly 11 years and indicating that high inflation was likely to persist as the pandemic pressures supply chains. read more .\n",
    "\n",
    "\"Today's data on wholesale prices should be eye-opening for the Federal Reserve, as inflation pressures still don't appear to be easing and will likely continue to be felt by the consumer in the coming months,\" said Charlie Ripley, senior investment strategist for Allianz Investment Management.\n",
    "\n",
    "Apple Inc (AAPL.O) fell 2.7% following a U.S. court ruling in \"Fortnite\" creator Epic Games' antitrust lawsuit that stroke down some of the iPhone maker's restrictions on how developers can collect payments in apps.\n",
    "\n",
    "\n",
    "Sponsored by Advertising Partner\n",
    "Sponsored Video\n",
    "Watch to learn more\n",
    "Report ad\n",
    "Apple shares were set for their worst single-day fall since May this year, weighing on the Nasdaq (.IXIC) and the S&P 500 technology sub-index (.SPLRCT), which fell 0.1%.\n",
    "\n",
    "Sentiment also took a hit from Cleveland Federal Reserve Bank President Loretta Mester's comments that she would still like the central bank to begin tapering asset purchases this year despite the weak August jobs report. read more\n",
    "\n",
    "Investors have paid keen attention to the labor market and data hinting towards higher inflation recently for hints on a timeline for the Federal Reserve to begin tapering its massive bond-buying program.\n",
    "\n",
    "The S&P 500 has risen around 19% so far this year on support from dovish central bank policies and re-opening optimism, but concerns over rising coronavirus infections and accelerating inflation have lately stalled its advance.\n",
    "\n",
    "\n",
    "Report ad\n",
    "The three main U.S. indexes got some support on Friday from news of a phone call between U.S. President Joe Biden and Chinese leader Xi Jinping that was taken as a positive sign which could bring a thaw in ties between the world's two most important trading partners.\n",
    "\n",
    "At 1:01 p.m. ET, the Dow Jones Industrial Average (.DJI) was up 12.24 points, or 0.04%, at 34,891.62, the S&P 500 (.SPX) was up 2.83 points, or 0.06%, at 4,496.11, and the Nasdaq Composite (.IXIC) was up 12.85 points, or 0.08%, at 15,261.11.\n",
    "\n",
    "Six of the eleven S&P 500 sub-indexes gained, with energy (.SPNY), materials (.SPLRCM) and consumer discretionary stocks (.SPLRCD) rising the most.\n",
    "\n",
    "U.S.-listed Chinese e-commerce companies Alibaba and JD.com , music streaming company Tencent Music (TME.N) and electric car maker Nio Inc (NIO.N) all gained between 0.7% and 1.4%\n",
    "\n",
    "\n",
    "Report ad\n",
    "Grocer Kroger Co (KR.N) dropped 7.1% after it said global supply chain disruptions, freight costs, discounts and wastage would hit its profit margins.\n",
    "\n",
    "Advancing issues outnumbered decliners by a 1.12-to-1 ratio on the NYSE and by a 1.02-to-1 ratio on the Nasdaq.\n",
    "\n",
    "The S&P index recorded 14 new 52-week highs and three new lows, while the Nasdaq recorded 49 new highs and 38 new lows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee6744",
   "metadata": {},
   "source": [
    "### Elaborating differences between tokenization and split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200140d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"It has been an amazing week by the grace of God!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0df7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'has',\n",
       " 'been',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'week',\n",
       " 'by',\n",
       " 'the',\n",
       " 'grace',\n",
       " 'of',\n",
       " 'God!!!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55fd822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'has',\n",
       " 'been',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'week',\n",
       " 'by',\n",
       " 'the',\n",
       " 'grace',\n",
       " 'of',\n",
       " 'God',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4 = nlp(msg)\n",
    "[token.text for token in doc4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c12198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split length: 11\n",
      "spaCy token length: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Split length: %d\\nspaCy token length: %d\" %(len(msg.split()), len(doc4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f96e1189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'go', 'to', 'the', 'market']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp(\"Let's go to the market\")\n",
    "[token.text for token in doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820704a",
   "metadata": {},
   "source": [
    "### Adding special rules of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba39165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389d8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lemme get that hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7707ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6273116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemme', 'get', 'that', 'hat']\n"
     ]
    }
   ],
   "source": [
    "print([w.text for w in doc6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7d8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding the tokenizer rule\n",
    "special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fc87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenizer rule adjustment:\n",
      "['lem', 'me', 'get', 'that', 'hat']\n"
     ]
    }
   ],
   "source": [
    "print(\"After tokenizer rule adjustment:\")\n",
    "print([w.text for w in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1b865",
   "metadata": {},
   "source": [
    "### Debugging the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c90191",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_exp = nlp.tokenizer.explain(\"Let's go now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7fc2161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let SPECIAL-1\n",
      "'s SPECIAL-2\n",
      "go TOKEN\n",
      "now TOKEN\n",
      "! SUFFIX\n"
     ]
    }
   ],
   "source": [
    "for w in token_exp:\n",
    "    print(w[1], w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0387ec",
   "metadata": {},
   "source": [
    "### spaCy sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86fff1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I took a bus to downtown.\n",
      "Then took a Toyo back home.\n"
     ]
    }
   ],
   "source": [
    "statement = \"I took a bus to downtown. Then took a Toyo back home.\"\n",
    "doc7 = nlp(statement)\n",
    "\n",
    "for sent in doc7.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f692c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object at 0x00000232998620E0>\n",
      "[I took a bus to downtown., Then took a Toyo back home.]\n"
     ]
    }
   ],
   "source": [
    "print(doc7.sents)\n",
    "print(list(doc7.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee027d",
   "metadata": {},
   "source": [
    "### Understanding Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad7898",
   "metadata": {},
   "source": [
    "A lemma is the base form of a token. You can think of a lemma as the form\n",
    "in which the token appears in a dictionary. For instance, the lemma of\n",
    "eating is eat; the lemma of eats is eat; ate similarly maps to eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2a8d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple --> Apple\n",
      "shares --> share\n",
      "were --> be\n",
      "set --> set\n",
      "for --> for\n",
      "their --> their\n",
      "worst --> bad\n",
      "single --> single\n",
      "- --> -\n",
      "day --> day\n",
      "fall --> fall\n",
      "since --> since\n",
      "May --> May\n",
      "this --> this\n",
      "year --> year\n"
     ]
    }
   ],
   "source": [
    "news = \"Apple shares were set for their worst single-day fall since May this year\"\n",
    "\n",
    "doc8 = nlp(news)\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text, '-->', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d02fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  ====>  I\n",
      "am  ====>  be\n",
      "going  ====>  go\n",
      "to  ====>  to\n",
      "visit  ====>  visit\n",
      "my  ====>  my\n",
      "father  ====>  father\n",
      "in  ====>  in\n",
      "Chuga  ====>  Arusha\n",
      ".  ====>  .\n",
      "Then  ====>  then\n",
      "my  ====>  my\n",
      "mother  ====>  mother\n",
      "at  ====>  at\n",
      "Bongo  ====>  Dar Es Salaam\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization in special case\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": 'Chuga'}]], {\"LEMMA\": 'Arusha'})\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": 'Bongo'}]], {\"LEMMA\": 'Dar Es Salaam'})\n",
    "# nlp.tokenizer.add_special_case('Chuga', new_rules)\n",
    "text = \"I am going to visit my father in Chuga. Then my mother at Bongo\"\n",
    "doc8 = nlp(text)\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text, ' ====> ', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d3b9737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[New York, Jordan]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dealing with named entities\n",
    "doc9 = nlp(\"I flew to New York with Jordan and Asha\")\n",
    "list(doc9.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d0a48d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, New York, Jordan, Asha]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Detecting nouns in a sentence\n",
    "list(doc9.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b54c9",
   "metadata": {},
   "source": [
    "## Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "17e90bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc9[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e482cb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc9[3].text_with_ws # token with white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e6d2c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc9[3].i # returns index of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0035713c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc9[3].idx # Position of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5c23743b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I flew to New York with Jordan and Asha"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc9[3]\n",
    "token.doc # accessing a doc that created a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6865566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if a token starts a sentence\n",
    "token.is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "782038cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2e9580b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Uhuru, Tanzania, last year)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc10 = nlp(\"President Uhuru visited Tanzania last year\")\n",
    "doc10.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59c77a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NORP'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc10[1].ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "63c309a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President \n",
      "Uhuru NORP\n",
      "visited \n",
      "Tanzania GPE\n",
      "last DATE\n",
      "year DATE\n"
     ]
    }
   ],
   "source": [
    "for token in doc10:\n",
    "    print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16a65d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Xxxxx\n",
      "Uhuru Xxxxx\n",
      "visited xxxx\n",
      "Tanzania Xxxxx\n",
      "last xxxx\n",
      "year xxxx\n"
     ]
    }
   ],
   "source": [
    "for token in doc10:\n",
    "    print(token, token.shape_) # capital letters are covered with X and small letters are in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c2db399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ----> True\n",
      "just ----> True\n",
      "wanted ----> False\n",
      "to ----> True\n",
      "say ----> True\n",
      "thank ----> False\n",
      "you ----> True\n",
      "! ----> False\n"
     ]
    }
   ],
   "source": [
    "# Tracking stop words (words that do not carry much meaning)\n",
    "doc11 = nlp(\"I just wanted to say thank you!\")\n",
    "for token in doc11:\n",
    "    print(token, '---->', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223c26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
